1
	1.1 - Course Intro
		3 main models we will look at
			Descriptive - What do neurons respond to?
			Mechanistic - How can we simulate?
			Interpretive - Why / Underlying principles?
	1.2 - Descriptive Models
		Receptive fields - sensory stimulus that generates strong response
			Center-surround response
		Cortical receptive fields
			Can be orientation specific
	1.3 - Mechanistic and Interpretive Models
		Retina -> Lateral Geniculate Nucleaus (LGN) -> Primary Visual Cortex (V1)
			Recurrent inputs to V1 cell as well as inputs from a pattern of LGN
		Cortical receptive fields are a way of efficiently compressing information in a natural image
	1.4 - Electrical Personality of Neurons
		Neuron
			25 um cell body
			dendrites - inputs
			axon - output
				myelin sheaths - action potential hops from node of Ranvier to node of Ranvier
					"active wire" lossless propagation
			lots of kinds of neurons
			"discrete and not continuous with other cells"
				largely true
			leaky bag of charged liquid
				lipid bilayer makes membrane
					ion channel in bilayer
						voltage gated
						chemically gated
						mechanically gated
				inside is ~-70mV relative to outside of cell
				Na, Cl, H20, K molecules and ions are major constituents
			synapse - interface between axon and cell body
	1.5 - Making Connections: Synapses
		Electrical synapse - gap junctions
			like a semiconductor
			fast connections
		Chemical synapse - neurotransmitters
			can have "weighting" in these synapses by augmenting number of receptors on receiving cell
		Can be excitatory or inhibitory
		Thought to be the basis for memory and learning
			Synaptic PLasticity
				Hebbian plasticity
					Neurons that fire together, wire together
					Long term potentiation/depression - synaptic response can stay for hours or days 
						Input/output spike timing
	1.6 - Network: Brain Areas and their Function
		Peripheral Nervous System
			Somatic - nerves connected to muscles and sensory receptors
				Afferent nerve fibers - inward
				Efferent nerve fibers - outward
			Autonomic - subconscious
		Central Nervous System 
			Spinal cord
				Local feedback - reflexes
				Descending motor control
				Ascending sensory axons
			Brain
				hindbrain
					medulla oblongata - regulates breathing etc
					pons - sleep+arousal
					cerebellum - timing voluntary movements, language, attention
				midbrain - sensory reflexes
				reticular formation - regulates
				thalamus - "relay station" for sensory
				hypothalamus - four Fs
				cerebrum
					 cerebral cortex - big dawg
					 	layered sheet of neurons
					 	30b neurons
					 	10k connections each
					 	300t total connections
					 basal ganglia
					 hippocampus
					 amygdala 
		Analogies
			Information storage - physical/chemical structure
			Information transmisison - Electrical/chemical signalling
			Primary compute element - neuron
			Computational basis - unknown
			
2
	2.1 - Neural Code
		Brain recording techniques
			fMRI
				inside big scanner
				blood O2
				cubic mm
				slow
			EEG
				hat with electrodes in contact with scalp
				electrical response in brain region
				noisy signal
				fast
			Electrode array
				individual neuron sized array
			Calcium imaging
				Imaging technique
			Patch electrodes
				inside cell - right in the ion channel
		Measuring neural code
			Functional map - shows response to stimulus across a spatial area
				Pinwheel features display highly specialized/localized regions of the tissue
				Can be feautures (line width, angle) or semantic structures (faces, houses)
			Tuning curve - response of single neuron by certain stimuli
				Shows "Brad Pitt and Jennifer Anniston" neuron
				Neurons are higher up in cortex, embody a concept over a feature
					Fire for images of person, text of their name, or audio recording of soeone saying their name
	2.2 - Neural Encoding: Simple Models
		Linear response
			r(t) = phi s(t)
			Doesn't care about recent inputs unless we add linear filter
				Can write as convolution
				Example: running average
				Example: Leaky average (slowly forgetting)
		Temporal/Spatial filtering
			As seen in retinal receptive fields
			Approximated by difference of multiple gaussians
		Spatiotemporal filtering
		When linear isn't enough
			Can approximate the system by wrapping the output with a nonlinear transform
		Can be described as a linear "feature extractor" and a nonlinear "map"
	2.3 - Neural Encoding: Feature Selection
		Need to find what drives the neuron in question
		In practice
			Feed in Gaussian white noise signal
			Plot a point corresponding to this signal in (t1,t2,t3,t4,...t99,...) space and note if it caused a spike or not
			Find average of all signals that caused the spike
		We can use "spike triggered average" as a filter by projecting input onto it
		PCA is good
	2.4 - Neural Encoding: Variability
		We want to define a mapping that provides maximal variability from stimulus and spike
			Kullback-Leibler divergence (DKL) is measure of goodness of f
		Binomial distribution
		Poisson's distribution
	Supplemental - Vectors and Functions
		Functions thought of as vectors because they can be described as long lists of numbers...
		Dot product of functions is integral of product of functions
	Supplemental - Convolutions and Linear Systems
		Linear System - Linear time invariant system (LTI)
		"Output is a filtered version of the input"
			"Output is the convolution of the filter with the input"
		Looking for parts of the signal that resemble the filter
			Highest output is part of the signal that most resembles the filter
	Supplemental - Change of Basis and PCA
3
	3.1 - Neural Decoding and Signal Detection Theory
		Likelihood ratio thresholding
		Evidence accumulates 
		Taking into account priors scales probability distributions as you would expect
	3.2 - Population Coding and Bayesian Estimation
		maximum likelihood
			responses weighted by inverse of variance, so sharp informative spikes are very influential
		maximum a posteriori (MAP)
		can be useful for the math to look at ln(probability)
	3.3 - Reading Minds: Stimulus Reconstruction
		Estimator minimzes squared error over all s
	3.4 - Fred Rieke on Visual Processing in the Retina
		Thousands of rods can detect a few photons
			Thresholding nonlinearity
			Threshold is far to the right, removes all possibility of noise
	Supplemental - Gaussians in One Dimension
		mu = mean
		sigma^2 = variance 
			sigma = stdev
		iid - idependent and identically distributed
		CLT - central limit theorem, sums + averages of any iid random variables are gaussian
		unimodal
		most entropy for given mean and variance
		fourier transform of a gaussian is a gaussian
		Do not use when:
			true distribution bimodal
			true distribution over small integers
			true distribution strictly positive
	Supplemental - Probability distribution in 2D and Bayes' Rule
		Joint - p(x,y) x AND y, function of x and y and distribution over x and y
		Marginal - p(x), function of x and distribution over x
		Conditional - p(x|y) x GIVEN y, function of x and y and distribution over x
		Independence - p(x,y) = p(x)p(y) or, equivalently p(x|y) = p(x)
		Chain rule - p(x,y) = p(x|y)p(y) = p(y|x)p(x)
		Marginalization rule - p(x) = int[p(x|y)p(y)dy]
		Bayes rule - p(x|y) = p(y|x)p(x)/p(y)
			p(x) - prior
			p(x|y) - posterior
			p(y|x) - likelihood
			p(y) - evidence
4
	4.1 - Information and Entropy
		Entropy = average information of random variable (number of yes no questions)
			H = - sum[p log2(p)]
		Mutual information = Total entropy - average noise entropy
			Recipe:
				Repeat stimulus s many times to obtain P(R|s)
				Compute variability due to noise:, noise entropy H[R|s]
				Repeat for all s and average sum(P(s)H[R|s])
				Copmute P(R) = sum(P(s)P(R|s)) and total entropy H[R]
	4.2 - Calculating Information in Spike Trains
		Spiketrain bin and word length are parameters that need to be chosen to capture information effectively
	4.3 - Coding Principles
		Natural stimuli has large dynamic range
		Input binning (in say a visual system) can remap based on the envelope
			stretching the input/output curve
		Principles of neural design:
			Efficiency
			Adaptation
			Sparseness
5
	5.1 - Modeling Neurons
		Lipid bilayer membrane - capacitor
		Ion channel - resistor
		Ion gradient - battery
		Nernst potential - potential where osmotic = electrostatic
		Equilibrium potentials are different for each type of ion channel:
			E_Na = 50mV
			E_Ca = 150mV
			E_K = -80mV
			E_Cl = -60mV
	5.2 - Spikes
		Channel resistance is voltage dependent
		Ion channel is made of subunits
			Current only flows if all are open
		Sodium channel can have inactivation gate
		Hodgkin + Huxley equation shows time varying voltage across system with K Na, and leak ion flows
7
	7.1 - Synaptic Plasticity, Hebb's Rule, Statistical Learning
		LTP/LTD - increase/decrease in synaptic strength for hours or days
			Results in increase/decrease of EPSP
		Hebb's rule - if A fires when B fires, the synapse is strengthened between the two
			What about LTD?
				Covariance based Hebb rule to update weights can address this
		Oja's rule - like Hebb's rule, but take a term off to keep the weights from blowing up
	7.2 - Intro to Unsupervised Learning
		Pick most likely, "winning" neuron
		Update weight by reassigning the cluster center to reflect new average
		Expectation-Maximization (EM) algorithm for updating G
			"soft", as opposed to "winner take all"
			All the points at once, or "batch" as opposed to "online"
	7.3 - Sparse and Predictive Coding
		Define your eigenspace and compress images by expressing just principal components
		Alternatively, you can use a space that is larger than the input to represent the feature basis. This results in  a flexible basis
8
	8.1 - Neurons as Classifiers and Supervised Learning
		Perceptron - thresholded weighted sum of +1/-1s
			can't do linearly unseparable data, like XOR
	8.2 - Reinforcement Learning: Predicting Rewards
	



https://journals.sagepub.com/doi/full/10.1177/1745691617709589#_i40